"""Level 8 Mini Capstone â€” full observability platform with KPIs, profiling, and fault injection.

Design rationale:
    This capstone integrates concepts from the entire level: KPI dashboards,
    response profiling, SLA monitoring, and fault injection. It builds a
    mini observability platform that monitors simulated services, detects
    degradation, and generates a unified health report.

Concepts practised:
    - integration of multiple subsystems
    - dataclasses for unified domain model
    - event-driven monitoring pipeline
    - threshold-based alerting
    - comprehensive reporting with drill-down
"""

from __future__ import annotations

import argparse
import json
import random
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any


# --- Domain types -------------------------------------------------------

class ServiceHealth(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    CRITICAL = "critical"
    DOWN = "down"


# WHY collect raw latency samples instead of pre-computed averages? -- Keeping
# individual measurements lets you compute any statistic after the fact:
# mean, percentiles, histograms. Pre-computing averages loses the distribution
# shape, making it impossible to spot tail latency issues later.
@dataclass
class ServiceMetrics:
    """Collected metrics for a single service."""
    name: str
    latency_ms: list[float] = field(default_factory=list)
    error_count: int = 0
    success_count: int = 0
    uptime_checks: int = 0
    uptime_passes: int = 0

    @property
    def request_count(self) -> int:
        return self.error_count + self.success_count

    @property
    def error_rate(self) -> float:
        if self.request_count == 0:
            return 0.0
        return self.error_count / self.request_count

    @property
    def availability(self) -> float:
        if self.uptime_checks == 0:
            return 100.0
        return self.uptime_passes / self.uptime_checks * 100

    @property
    def p50_latency(self) -> float:
        return _percentile(self.latency_ms, 50)

    @property
    def p95_latency(self) -> float:
        return _percentile(self.latency_ms, 95)

    @property
    def p99_latency(self) -> float:
        return _percentile(self.latency_ms, 99)

    def health(self) -> ServiceHealth:
        if self.availability < 95:
            return ServiceHealth.DOWN
        if self.error_rate > 0.10 or self.p99_latency > 1000:
            return ServiceHealth.CRITICAL
        if self.error_rate > 0.05 or self.p99_latency > 500:
            return ServiceHealth.DEGRADED
        return ServiceHealth.HEALTHY

    def to_dict(self) -> dict[str, Any]:
        return {
            "name": self.name,
            "requests": self.request_count,
            "error_rate_pct": round(self.error_rate * 100, 2),
            "availability_pct": round(self.availability, 2),
            "p50_ms": round(self.p50_latency, 1),
            "p95_ms": round(self.p95_latency, 1),
            "p99_ms": round(self.p99_latency, 1),
            "health": self.health().value,
        }


@dataclass
class Alert:
    """An alert generated by the monitoring system."""
    service: str
    severity: str
    message: str
    timestamp: float = field(default_factory=time.time)

    def to_dict(self) -> dict[str, Any]:
        return {
            "service": self.service,
            "severity": self.severity,
            "message": self.message,
        }


@dataclass
class PlatformReport:
    """Unified platform health report."""
    services: list[ServiceMetrics]
    alerts: list[Alert]
    overall_health: ServiceHealth

    def to_dict(self) -> dict[str, Any]:
        return {
            "overall_health": self.overall_health.value,
            "service_count": len(self.services),
            "alert_count": len(self.alerts),
            "services": [s.to_dict() for s in self.services],
            "alerts": [a.to_dict() for a in self.alerts[:20]],
            "summary": {
                "healthy": sum(1 for s in self.services if s.health() == ServiceHealth.HEALTHY),
                "degraded": sum(1 for s in self.services if s.health() == ServiceHealth.DEGRADED),
                "critical": sum(1 for s in self.services if s.health() == ServiceHealth.CRITICAL),
                "down": sum(1 for s in self.services if s.health() == ServiceHealth.DOWN),
            },
        }


# --- Helpers ------------------------------------------------------------

def _percentile(values: list[float], pct: float) -> float:
    if not values:
        return 0.0
    s = sorted(values)
    import math
    idx = max(0, math.ceil(pct / 100 * len(s)) - 1)
    return s[idx]


# --- Observability platform ---------------------------------------------

class ObservabilityPlatform:
    """Mini observability platform integrating metrics, alerts, and health."""

    def __init__(self, service_names: list[str]) -> None:
        self._metrics: dict[str, ServiceMetrics] = {
            name: ServiceMetrics(name=name) for name in service_names
        }
        self._alerts: list[Alert] = []

    def record_request(
        self, service: str, latency_ms: float, success: bool,
    ) -> None:
        """Record a single request to a service."""
        metrics = self._metrics.get(service)
        if not metrics:
            return
        metrics.latency_ms.append(latency_ms)
        if success:
            metrics.success_count += 1
        else:
            metrics.error_count += 1

    def record_health_check(self, service: str, passed: bool) -> None:
        """Record a synthetic health check result."""
        metrics = self._metrics.get(service)
        if not metrics:
            return
        metrics.uptime_checks += 1
        if passed:
            metrics.uptime_passes += 1

    def evaluate_alerts(self) -> list[Alert]:
        """Check all services and generate alerts for degraded ones."""
        new_alerts: list[Alert] = []
        for metrics in self._metrics.values():
            health = metrics.health()
            if health == ServiceHealth.CRITICAL:
                alert = Alert(
                    service=metrics.name, severity="critical",
                    message=f"{metrics.name}: error_rate={metrics.error_rate:.1%}, "
                            f"p99={metrics.p99_latency:.0f}ms",
                )
                new_alerts.append(alert)
                self._alerts.append(alert)
            elif health == ServiceHealth.DEGRADED:
                alert = Alert(
                    service=metrics.name, severity="warning",
                    message=f"{metrics.name}: degraded performance",
                )
                new_alerts.append(alert)
                self._alerts.append(alert)
            elif health == ServiceHealth.DOWN:
                alert = Alert(
                    service=metrics.name, severity="page",
                    message=f"{metrics.name}: service DOWN, "
                            f"availability={metrics.availability:.1f}%",
                )
                new_alerts.append(alert)
                self._alerts.append(alert)
        return new_alerts

    def report(self) -> PlatformReport:
        """Generate a unified platform health report."""
        services = list(self._metrics.values())
        healths = [s.health() for s in services]

        if ServiceHealth.DOWN in healths:
            overall = ServiceHealth.DOWN
        elif ServiceHealth.CRITICAL in healths:
            overall = ServiceHealth.CRITICAL
        elif ServiceHealth.DEGRADED in healths:
            overall = ServiceHealth.DEGRADED
        else:
            overall = ServiceHealth.HEALTHY

        return PlatformReport(
            services=services,
            alerts=self._alerts,
            overall_health=overall,
        )


# --- Simulation ---------------------------------------------------------

def run_simulation(
    num_requests: int = 200,
    seed: int = 42,
) -> dict[str, Any]:
    """Simulate traffic to multiple services with varying health."""
    rng = random.Random(seed)

    service_profiles = {
        "api-gateway": {"latency_base": 50, "error_prob": 0.02, "down_prob": 0.0},
        "user-service": {"latency_base": 30, "error_prob": 0.01, "down_prob": 0.0},
        "payment-service": {"latency_base": 200, "error_prob": 0.08, "down_prob": 0.01},
        "search-service": {"latency_base": 100, "error_prob": 0.03, "down_prob": 0.0},
        "notification-svc": {"latency_base": 80, "error_prob": 0.15, "down_prob": 0.02},
    }

    platform = ObservabilityPlatform(list(service_profiles.keys()))

    for _ in range(num_requests):
        for svc_name, profile in service_profiles.items():
            latency = rng.gauss(profile["latency_base"], profile["latency_base"] * 0.3)
            latency = max(1, latency)
            success = rng.random() > profile["error_prob"]
            platform.record_request(svc_name, latency, success)

            health_pass = rng.random() > profile["down_prob"]
            platform.record_health_check(svc_name, health_pass)

    platform.evaluate_alerts()
    return platform.report().to_dict()


# --- CLI ----------------------------------------------------------------

def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Level 8 Capstone: Observability Platform")
    parser.add_argument("--requests", type=int, default=200, help="Simulated requests per service")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    return parser.parse_args(argv)


def main(argv: list[str] | None = None) -> None:
    args = parse_args(argv)
    output = run_simulation(num_requests=args.requests, seed=args.seed)
    print(json.dumps(output, indent=2))


if __name__ == "__main__":
    main()
